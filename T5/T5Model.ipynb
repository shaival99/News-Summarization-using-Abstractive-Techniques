{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T5Model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "23148702a56245059b3ca61243268b0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3a0c08be9a5b4ea59dce2d421616f938",
              "IPY_MODEL_66bc0891e31d4bc0a8ba2a2d72200021"
            ],
            "layout": "IPY_MODEL_d8c591186d694acf98b84a66ad62ef54"
          }
        },
        "3a0c08be9a5b4ea59dce2d421616f938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96b3649475474bca8e23d9ffb7a0cb66",
            "placeholder": "​",
            "style": "IPY_MODEL_51d8d4e27e31475f8d6156e1661a593e",
            "value": "1.083 MB of 1.083 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "66bc0891e31d4bc0a8ba2a2d72200021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_72a162bd22654324a020d504e356758a",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_70efdb700cfd4e9ba9b12af8db1883e7",
            "value": 1
          }
        },
        "d8c591186d694acf98b84a66ad62ef54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96b3649475474bca8e23d9ffb7a0cb66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51d8d4e27e31475f8d6156e1661a593e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "72a162bd22654324a020d504e356758a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70efdb700cfd4e9ba9b12af8db1883e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install wandb -q"
      ],
      "metadata": {
        "id": "poOZ7A8BxdT8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "654e43ae-c5f8-4a98-c328-21ba93d007c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.0 MB 5.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 57.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 880 kB 55.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 62.1 MB/s \n",
            "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 5.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 73.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 71.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing stock libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# Importing the T5 modules from huggingface/transformers\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# WandB – Import the wandb library\n",
        "import wandb\n",
        "\n",
        "import transformers\n",
        "from datasets import load_dataset\n",
        "from torch.optim import Adam\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset,RandomSampler,SequentialSampler\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer,T5PreTrainedModel # SentencePiece library is required to download pretrained t5tokenizer\n",
        "# Let's try T5TokenizerFast\n",
        "from transformers.models.t5 import T5TokenizerFast"
      ],
      "metadata": {
        "id": "hfRUlAedyNin"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b6eLqa2908ul",
        "outputId": "5980ba67-b891-47e2-8321-3817cfe7b49b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rouge-score\n",
            "  Downloading rouge_score-0.0.4-py2.py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.21.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.0.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Installing collected packages: rouge-score\n",
            "Successfully installed rouge-score-0.0.4\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.1 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 65.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 71.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 68.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.2.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 71.0 MB/s \n",
            "\u001b[?25hCollecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 76.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 75.0 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "urllib3"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_metric\n",
        "# from rouge import Rouge\n",
        "metric = load_metric(\"rouge\")"
      ],
      "metadata": {
        "id": "pcpe2p9b0_ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rOhk_NYySKX",
        "outputId": "f80f2d42-25c0-4ee6-b317-c99b25ff7da8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu May  5 15:37:03 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "3FzLTHEtyVEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X8s9KhgKycDq",
        "outputId": "b7f7e4db-f3c3-40a6-c5ca-8dfe6faaa3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mshaivals\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = dataframe\n",
        "        self.source_len = source_len\n",
        "        self.summ_len = summ_len\n",
        "        self.text = self.data.article\n",
        "        self.ctext = self.data.highlights\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        ctext = str(self.ctext[index])\n",
        "        ctext = ' '.join(ctext.split())\n",
        "\n",
        "        text = str(self.text[index])\n",
        "        text = ' '.join(text.split())\n",
        "\n",
        "        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n",
        "        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n",
        "\n",
        "        source_ids = source['input_ids'].squeeze()\n",
        "        source_mask = source['attention_mask'].squeeze()\n",
        "        target_ids = target['input_ids'].squeeze()\n",
        "        target_mask = target['attention_mask'].squeeze()\n",
        "\n",
        "        return {\n",
        "            'source_ids': source_ids.to(dtype=torch.long), \n",
        "            'source_mask': source_mask.to(dtype=torch.long), \n",
        "            'target_ids': target_ids.to(dtype=torch.long),\n",
        "            'target_ids_y': target_ids.to(dtype=torch.long)\n",
        "        }"
      ],
      "metadata": {
        "id": "WR7C39DLyeKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch, tokenizer, model, device, loader, optimizer):\n",
        "    model.train()\n",
        "    for _,data in enumerate(loader, 0):\n",
        "        y = data['target_ids'].to(device, dtype = torch.long)\n",
        "        y_ids = y[:, :-1].contiguous()\n",
        "        lm_labels = y[:, 1:].clone().detach()\n",
        "        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100 #Here, we make sure that padding token id’s of the labels are not taken into account by the loss function, by replacing them with -100.\n",
        "        ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "        mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, labels=lm_labels)\n",
        "        # print(outputs[0]) #This just prints the tensor values of the NllLoss at each iteration in an epoch.\n",
        "        loss = outputs[0]\n",
        "        \n",
        "        if _%10 == 0:\n",
        "            wandb.log({\"Training Loss\": loss.item()})\n",
        "\n",
        "        if _%500==0:\n",
        "            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "id": "F2XiYUJByoiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(epoch, tokenizer, model, device, loader):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for _, data in enumerate(loader, 0):\n",
        "            y = data['target_ids'].to(device, dtype = torch.long)\n",
        "            ids = data['source_ids'].to(device, dtype = torch.long)\n",
        "            mask = data['source_mask'].to(device, dtype = torch.long)\n",
        "\n",
        "            generated_ids = model.generate(\n",
        "                input_ids = ids,\n",
        "                attention_mask = mask, \n",
        "                max_length=100, \n",
        "                num_beams=4,\n",
        "                repetition_penalty=2.5, \n",
        "                length_penalty=1.0, \n",
        "                early_stopping=True\n",
        "                )\n",
        "            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n",
        "            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n",
        "            if _%100==0:\n",
        "                print(f'Completed {_}')\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            actuals.extend(target)\n",
        "    return predictions, actuals"
      ],
      "metadata": {
        "id": "dkKCnmpNyrby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # WandB – Initialize a new run\n",
        "    wandb.init(project=\"Summarization\")\n",
        "\n",
        "    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n",
        "    # Defining some key variables that will be used later on in the training  \n",
        "    config = wandb.config          # Initialize config\n",
        "    config.TRAIN_BATCH_SIZE = 2    # input batch size for training\n",
        "    config.VALID_BATCH_SIZE = 2   # input batch size for testing\n",
        "    config.TRAIN_EPOCHS = 2        \n",
        "    config.VAL_EPOCHS = 1 \n",
        "    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01) This is generally high for T5 when using Adam optimizers(in the range of 1e-4 to 3e-4)\n",
        "    config.SEED = 42              \n",
        "    config.MAX_LEN = 900\n",
        "    config.SUMMARY_LEN = 100 \n",
        "\n",
        "    # Set random seeds and deterministic pytorch for reproducibility\n",
        "    torch.manual_seed(config.SEED) # pytorch random seed\n",
        "    np.random.seed(config.SEED) # numpy random seed\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "    # tokenzier for encoding the text\n",
        "    tokenizer = T5TokenizerFast.from_pretrained('t5-base') #Here, we used the FastTokenizer in order to focus only on Unigrams. It is faster than the normal one\n",
        "    \n",
        "\n",
        " \n",
        "    # Selecting the needed columns only. \n",
        "    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n",
        "    df = pd.read_csv('/content/drive/MyDrive/DL Project/train.csv',encoding='latin-1')\n",
        "    df = df.sample(frac=0.001)\n",
        "    df = df.drop(columns=['id'])\n",
        "    df = df[['article','highlights']]\n",
        "    df.highlights = 'summarize: ' + df.highlights\n",
        "    # print(df.head())\n",
        "\n",
        "    \n",
        "    # Creation of Dataset and Dataloader\n",
        "    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n",
        "    train_size = 0.8\n",
        "    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n",
        "    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n",
        "    train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "    print(\"FULL Dataset: {}\".format(df.shape))\n",
        "    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n",
        "\n",
        "\n",
        "    # Creating the Training and Validation dataset for further creation of Dataloader\n",
        "    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n",
        "\n",
        "    # Defining the parameters for creation of dataloaders\n",
        "    train_params = {\n",
        "        'batch_size': config.TRAIN_BATCH_SIZE,\n",
        "        'shuffle': True,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "    val_params = {\n",
        "        'batch_size': config.VALID_BATCH_SIZE,\n",
        "        'shuffle': False,\n",
        "        'num_workers': 0\n",
        "        }\n",
        "\n",
        "    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n",
        "    training_loader = DataLoader(training_set, **train_params)\n",
        "    val_loader = DataLoader(val_set, **val_params)\n",
        "\n",
        "\n",
        "    \n",
        "    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n",
        "    # Further this model is sent to device (GPU/TPU) for using the hardware.\n",
        "    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n",
        "    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n",
        "\n",
        "    # Log metrics with wandb\n",
        "    wandb.watch(model, log=\"all\")\n",
        "    # Training loop\n",
        "    print('Initiating Fine-Tuning for the model on our dataset')\n",
        "\n",
        "    for epoch in range(config.TRAIN_EPOCHS):\n",
        "        train(epoch, tokenizer, model, device, training_loader, optimizer)\n",
        "\n",
        "\n",
        "    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n",
        "    # Saving the dataframe as predictions.csv\n",
        "    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n",
        "    for epoch in range(config.VAL_EPOCHS):\n",
        "        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n",
        "        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n",
        "        final_df.to_csv('/content/drive/MyDrive/DL Project/predictions_T5.csv')\n",
        "        print('Output Files generated for review')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "23148702a56245059b3ca61243268b0d",
            "3a0c08be9a5b4ea59dce2d421616f938",
            "66bc0891e31d4bc0a8ba2a2d72200021",
            "d8c591186d694acf98b84a66ad62ef54",
            "96b3649475474bca8e23d9ffb7a0cb66",
            "51d8d4e27e31475f8d6156e1661a593e",
            "72a162bd22654324a020d504e356758a",
            "70efdb700cfd4e9ba9b12af8db1883e7"
          ]
        },
        "id": "cwG2DbW1yt1O",
        "outputId": "ed415e10-60ff-48cf-c5f9-6d67fb34f1b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing last run (ID:2ls1l28v) before initializing another..."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "23148702a56245059b3ca61243268b0d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>█▅▆▇▅▃▆▃▅▆▄▅▅▄▅▄▄▅▆▇▁▁▂▄▁▃▃▄▄▃▃▂▄▂▃▂▃▄▂▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Training Loss</td><td>2.97591</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Synced <strong style=\"color:#cdcd00\">ancient-emperor-6</strong>: <a href=\"https://wandb.ai/shaivals/Summarization/runs/2ls1l28v\" target=\"_blank\">https://wandb.ai/shaivals/Summarization/runs/2ls1l28v</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20220505_230156-2ls1l28v/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Successfully finished last run (ID:2ls1l28v). Initializing new run:<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.12.16"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20220505_233005-2e0beu0b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href=\"https://wandb.ai/shaivals/Summarization/runs/2e0beu0b\" target=\"_blank\">legendary-bothan-7</a></strong> to <a href=\"https://wandb.ai/shaivals/Summarization\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (287, 2)\n",
            "TRAIN Dataset: (230, 2)\n",
            "TEST Dataset: (57, 2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initiating Fine-Tuning for the model on our dataset\n",
            "tensor(6.9332, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 0, Loss:  6.933244705200195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2269: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.4807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(5.6229, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(5.8004, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(5.5671, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(5.4323, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(5.1789, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6875, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2747, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.5077, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.8763, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5678, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6654, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2479, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2592, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6002, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.3436, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1319, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4693, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8087, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7304, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8925, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6243, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6187, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5874, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6733, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5621, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8841, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4035, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5053, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3452, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2645, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6768, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5558, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.1408, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6370, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.6425, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9322, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4660, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5626, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7721, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3807, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1754, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3422, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.2223, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6414, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5981, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0935, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8256, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6992, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3014, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6353, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8418, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5937, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8465, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6737, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3764, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5758, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7930, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8390, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5759, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5624, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9351, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3882, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1457, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8406, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7285, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8184, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7913, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9956, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4428, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1108, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4694, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1172, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.9579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.7868, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5853, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4396, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5679, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8785, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4903, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4506, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1934, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3655, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9587, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1686, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6971, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2288, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6888, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2277, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1215, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5361, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0534, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(4.4972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2245, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7311, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9216, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4836, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.8155, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2274, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3602, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Epoch: 1, Loss:  2.360227584838867\n",
            "tensor(3.0330, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4616, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2724, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2463, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5492, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1880, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8921, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2030, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4341, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9473, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5333, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7636, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9062, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4915, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6943, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4169, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9583, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2204, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8549, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9760, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8307, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5756, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4912, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3543, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6195, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6729, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.8975, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9547, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7794, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4442, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4629, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2780, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0232, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3364, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8818, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1576, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1550, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7331, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6565, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5072, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2994, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8960, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2150, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2355, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1402, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5696, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2273, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9697, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9828, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2858, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.6894, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.2759, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7823, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4859, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6163, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1970, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8972, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.1069, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1354, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9283, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3224, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8200, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5076, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1154, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8421, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0782, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3809, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0870, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.1658, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(1.9298, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3340, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6225, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8373, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9744, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.8713, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5083, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6692, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5214, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6241, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3761, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2495, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3337, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.1424, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4013, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6885, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.3738, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.3774, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2614, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2579, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.4262, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9987, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4708, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4766, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.4582, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2490, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5798, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.5335, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6454, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6996, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.0787, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.5762, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.9573, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.2881, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7444, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6753, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(3.1650, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.6557, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "tensor(2.7345, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe\n",
            "Completed 0\n",
            "Output Files generated for review\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentencepiece"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rpO6uzU7CNFL",
        "outputId": "7db74211-b9df-4435-ab4f-e9ef997c155b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 21.6 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 7.1 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 9.3 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 8.4 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 8.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 8.6 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/DL Project/predictions_T5.csv\")"
      ],
      "metadata": {
        "id": "vU6WHPyG1anJ"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "FIYxdxZs1upm",
        "outputId": "23225baf-34a5-41aa-c7a2-8bb06bf78fa5"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Unnamed: 0                                     Generated Text  \\\n",
              "0           0  PUBLISHED:. 06:56 EST, 30 May 2013. |. UPDATED...   \n",
              "1           1  PUBLISHED:. 08:56 EST, 30 September 2013. |. U...   \n",
              "2           2  Morrisons is to sell ready-peeled fruits in th...   \n",
              "3           3  Hundreds of Egyptians have died on election da...   \n",
              "4           4  'Selfie', 'twerk' and 'Twittersphere' are amon...   \n",
              "\n",
              "                                         Actual Text  \n",
              "0  (CNN) -- PepsiCo is ending its relationship wi...  \n",
              "1  By. Tara Brady. PUBLISHED:. 00:21 EST, 3 March...  \n",
              "2  By. Daily Mail Reporter. UPDATED:. 07:24 EST, ...  \n",
              "3  Cairo (CNN) -- Violence marked the beginning o...  \n",
              "4  'Selfie' may have been named Oxford. Dictionar...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-54b20481-403a-4133-abcc-a6ea9bb88d81\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Generated Text</th>\n",
              "      <th>Actual Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>PUBLISHED:. 06:56 EST, 30 May 2013. |. UPDATED...</td>\n",
              "      <td>(CNN) -- PepsiCo is ending its relationship wi...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>PUBLISHED:. 08:56 EST, 30 September 2013. |. U...</td>\n",
              "      <td>By. Tara Brady. PUBLISHED:. 00:21 EST, 3 March...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>Morrisons is to sell ready-peeled fruits in th...</td>\n",
              "      <td>By. Daily Mail Reporter. UPDATED:. 07:24 EST, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>Hundreds of Egyptians have died on election da...</td>\n",
              "      <td>Cairo (CNN) -- Violence marked the beginning o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>'Selfie', 'twerk' and 'Twittersphere' are amon...</td>\n",
              "      <td>'Selfie' may have been named Oxford. Dictionar...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54b20481-403a-4133-abcc-a6ea9bb88d81')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-54b20481-403a-4133-abcc-a6ea9bb88d81 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-54b20481-403a-4133-abcc-a6ea9bb88d81');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kJYeFvT1vdx",
        "outputId": "14c97ff5-be27-4649-a50c-738814ccb2f8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5742, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_text = df[\"Generated Text\"].tolist()\n",
        "actual_text = df[\"Actual Text\"].tolist()"
      ],
      "metadata": {
        "id": "Wktc0uZS1yEp"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9X4dV4s17gu",
        "outputId": "4893dc30-e823-4b85-9f12-f222bca31477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.7/dist-packages (0.0.4)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.21.6)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from rouge-score) (3.2.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from rouge-score) (1.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_metric"
      ],
      "metadata": {
        "id": "suXtwVFG1_Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_scores = load_metric(\"rouge\")"
      ],
      "metadata": {
        "id": "rXhNR9qZ2DG-"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Calculating Average Rouge-1, Rouge-2 and Rouge-L scores"
      ],
      "metadata": {
        "id": "mvICqWZY0SsN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = rouge_scores.compute(predictions=predicted_text, references=actual_text)"
      ],
      "metadata": {
        "id": "mYX1mARe2G1K"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Rouge-1 Score of T5 Model:\", results[\"rouge1\"].high.fmeasure, \"\\nRouge-2 Score of T5 Model:\", results[\"rouge2\"].high.fmeasure, \"\\nRouge-L Score of T5 Model:\", results[\"rougeL\"].high.fmeasure)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qbZuiza1CX3",
        "outputId": "9357bad9-34aa-4133-d3eb-b38d854c4483"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rouge-1 Score of T5 Model: 0.3460494998534182 \n",
            "Rouge-2 Score of T5 Model: 0.12486014352479728 \n",
            "Rouge-L Score of T5 Model: 0.2153263670523677\n"
          ]
        }
      ]
    }
  ]
}